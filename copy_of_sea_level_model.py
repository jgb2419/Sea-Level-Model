# -*- coding: utf-8 -*-
"""Copy of Sea Level Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yVX7_HXrAiM1vqTMlUmeYbDVQnlyO6_X
"""

!pip install chainconsumer
!pip install -U pip
!pip install -U setuptools setuptools_scm pep517
!pip install -U emcee
!pip install pyDOE

# Commented out IPython magic to ensure Python compatibility.
import emcee
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt
import statsmodels.api as sm
from chainconsumer import ChainConsumer
from pyDOE import lhs
# %matplotlib inline
import csv
import pandas as pd

from google.colab import drive
#drive.mount('/drive')
#dfS = pd.read_csv('/drive/My Drive/data_temperature_sealevel.csv')
dfS = pd.read_csv("https://raw.githubusercontent.com/tonyewong/risk_analysis_course_spring2022/main/data_temperature_sealevel.csv")

np.random.seed(3202)

dfS

#uncertainty
sigma = dfS['sigma_sealevel']
#x_grid is the years
years = dfS['year']

#the actual sea level depths
real_sea_levels = dfS['sealevel']
plt.scatter(years, real_sea_levels, color='coral', label='$y_{meas}$')
plt.xlabel('Years')
plt.ylabel('Sea Levels')
plt.legend()
plt.show()

def gmsl_model(a=3.4, Teq=-0.5, S0=0, T_forcing=None, dt=1):
    n_time = len(T_forcing)
    S = np.zeros(n_time)
    S[0] = S0
    for t in range(n_time-1):
        S[t+1] = S[t] + dt*a*(T_forcing[t]-Teq)
    return S

#T_1951_1980 is mean temp from 1980-1951
#offsetting time series so it has a mean of 0 in 1951-1980
T_1951_1980 = dfS.loc[(dfS["year"] >= 1951) & (dfS["year"] <= 1980), "temperature"].mean()
dfS["temperature"] = dfS["temperature"] - T_1951_1980

#normalized sea levels from 1951-1980
S_1951_1980 = dfS.loc[(dfS["year"] >= 1951) & (dfS["year"] <= 1980), "sealevel"].mean()
dfS["sealevel"] = dfS["sealevel"] - S_1951_1980

#initial sea level
S0 = dfS["sealevel"][0]

#model
S = gmsl_model(a=3.5, Teq=-0.51, S0=S0, T_forcing=dfS["temperature"], dt=1)

#the actual sea level depths
years = dfS['year']
sigma = dfS['sigma_sealevel']

real_sea_levels = dfS['sealevel']
plt.plot(years, S, label='$y_{true}$')
plt.scatter(years, real_sea_levels, color='coral', label='$y_{meas}$')
plt.xlabel('Years')
plt.ylabel('Sea Levels')
plt.legend()
plt.show()

def likelihood(parameter):
    #parameter = [alpha, beta]
    prod = 1
    for i in range(len(years)):
        delta = (parameter[0]*years[i] + parameter[1]) - real_sea_levels[i]
        prod *= stats.norm.pdf(x=delta, loc=0, scale=sigma)
    return prod

def loglikelihood(parameter, temperature, real_sea_levels, years, sigmas):
  # run the model                v--------v---------v------parameters should be from the `parameter` input
  #pred_sea_levels = gmsl_model(a=3.6, Teq=-0.45, S0=S0, T_forcing=temperature, dt=1)
  pred_sea_levels = gmsl_model(a = parameter[0], Teq=parameter[1], S0=parameter[2], T_forcing=temperature, dt=1)

  prod = 1
  for i in range(len(years)):
    delta = pred_sea_levels[i] - real_sea_levels[i]
    prod += stats.norm.logpdf(x=delta, loc=0, scale=sigmas[i])
  return prod

loglikelihood([3, -0.45, -100], dfS["temperature"], real_sea_levels, years, sigma)

"""$log(f(a,b)) = Log(f(a)*f(b)) = log(f(a)) + log(f(b))$

alpha could be normal, centered at 3.4, with stdev 1

Teq could be normal, centered at 0, with stdev 1

S0 could be normal, centered at the first sea level data point, with stdev = the first sea level sigma (uncertainty)
"""

#Normal Distribution
def prior(parameter):
  alpha_norm = stats.norm.pdf(parameter[0], 0, 10)
  beta_norm = stats.norm.pdf(parameter[1], 0, 10)
  return alpha_norm * beta_norm

#Normal Distribution
def logprior(parameter):
  alpha_norm = stats.norm.logpdf(parameter[0], 0, 10)
  Teq_norm = stats.norm.logpdf(parameter[1], 0, 10)
  S0_norm = stats.norm.logpdf(parameter[2], 0, 10)
  return alpha_norm + Teq_norm + S0_norm

def posterior(parameter):
    pri = prior(parameter)
    if pri==0:
        return 0
    else:
        return likelihood(parameter)*pri

def logposterior(parameter, temperature, real_sea_levels, years, sigma):
    logpri = logprior(parameter)
    if logpri==float(-np.inf):
        return 0
    else:
        return loglikelihood(parameter, temperature, real_sea_levels, years, sigma)+logpri

logposterior([3, -0.45, -100], dfS["temperature"], real_sea_levels, years, sigma)

nwalkers = 12
initial_parameters = np.array([3.6, -0.5, S0])
ndim = len(initial_parameters)
sampler = emcee.EnsembleSampler(nwalkers, ndim, logposterior, args=[dfS['temperature'], real_sea_levels, years, sigma])

p0 = [initial_parameters + 1e-1 * np.random.randn(ndim) for i in range(nwalkers)]

p0

thing1, prob, state = sampler.run_mcmc(p0, 500)
# thing1 = parameter estimates at the end of each chain
# prob = the log-posterior value from the end of each chain
# state = scary

thing1

sampler.reset()
niter = 5000
pos, prob, state = sampler.run_mcmc(thing1, niter)

pos

samples = sampler.get_chain(flat=False)
samples_pooled = np.concatenate((samples[:,0,:],samples[:,1,:],samples[:,2,:],samples[:,3,:],samples[:,4,:],samples[:,5,:], samples[:,6,:],samples[:,7,:],samples[:,8,:],samples[:,9,:],samples[:,10,:],samples[:,11,:]))

samples_pooled.shape

plt.hist(samples_pooled[:,2])

plt.plot(samples[:,0,0])
plt.plot(samples[:,1,0])
plt.plot(samples[:,2,0])
plt.plot(samples[:,3,0])
plt.plot(samples[:,4,0])
plt.plot(samples[:,5,0])

#GR diagnostics - check for convergence
alpha_chain = ChainConsumer()
alpha_chain.add_chain(samples_pooled[:,0], walkers = 12, name = "alpha")
gelman_rubin_converged_alpha = alpha_chain.diagnostic.gelman_rubin()

Teq_chain = ChainConsumer()
Teq_chain.add_chain(samples_pooled[:,1], walkers = 12, name = "Teq")
gelman_rubin_converged_Teq = Teq_chain.diagnostic.gelman_rubin()

S0_chain = ChainConsumer()
S0_chain.add_chain(samples_pooled[:,2], walkers = 12, name="S0")
gelman_rubin_converged_S0 = S0_chain.diagnostic.gelman_rubin()

num_lags = 0
for params in range(12):
  for walks in range(3):
    acfs = sm.tsa.stattools.acf(samples[:,params,walks], nlags=3000)
    lags = np.where(np.abs(acfs) < 0.05)[0][0]
    if lags > num_lags:
      num_lags = lags

num_lags

thinned_alpha = []
thinned_Teq = []
thinned_S0 = []
alphas = samples_pooled[:,0]
Teqs = samples_pooled[:,1]
S0s = samples_pooled[:,2]

for num in range(len(alphas)):
  if num % num_lags == 0:
    thinned_alpha.append(alphas[num])
    thinned_Teq.append(Teqs[num])
    thinned_S0.append(S0s[num])
plt.hist(thinned_alpha, density=True, alpha = 0.5, bins=20)
plt.hist(thinned_Teq, density=True, alpha = 0.5, bins=20)

"""Add S0 to these plots. have a plot for alpha vs thinned alpha (which you have!), and then same for Teq, and for S0"""

plt.hist(thinned_Teq, density=True, alpha = 0.5, bins=20, label="with thinning")
plt.hist(Teqs, density=True, alpha = 0.5, bins=20, label="no thinning")
plt.legend()

plt.hist(thinned_alpha, density=True, alpha = 0.5, bins=20, label="with thinning")
plt.hist(alphas, density=True, alpha = 0.5, bins=20, label="no thinning")
plt.legend()

plt.hist(thinned_S0, density=True, alpha = 0.5, bins=20, label="with thinning")
plt.hist(S0s, density=True, alpha = 0.5, bins=20, label="no thinning")
plt.legend()

np.percentile(thinned_alpha, [5, 95])

np.percentile(alphas, [5, 95])

"""# TODO:

use the sneasy_temperature_RCP60_...csv file for the temperatures

you'll need to normalize them to the same reference period as the temperatures and sea levels earlier

for each of the thinned sets of model parameters, run the `gmsl_model` and save the sea levels in the year 2100, plot a density histogram of those

for each of the sets of model parameters, run the `gmsl_model` and save the sea levels in the year 2100, plot a density histogram of those

compare the sea levels in 2100
"""

#sneasy_temp = pd.read_csv('/drive/My Drive/sneasy_temperature_RCP60_1850_2300.csv')
sneasy_temp = pd.read_csv("https://raw.githubusercontent.com/tonyewong/risk_analysis_course_spring2022/main/sneasy_temperature_RCP60_1850_2300.csv")

T_1951_1980 = sneasy_temp.loc[(sneasy_temp['Year'] >= 1951) & (sneasy_temp["Year"] <= 1980), "MAP Temperature"].mean()
sneasy_temp["MAP Temperature"] = sneasy_temp["MAP Temperature"] - T_1951_1980

# initialize an array to store all the sea level projections
pred_sea_levels_thin = np.zeros((len(thinned_alpha), len(sneasy_temp)))
pred_sea_levels_nothin = np.zeros((len(alphas), len(sneasy_temp)))

for x in range(len(thinned_alpha)):
    pred_sea_levels_thin[x,:] = gmsl_model(a = thinned_alpha[x], Teq=thinned_Teq[x], S0=thinned_S0[x], T_forcing=sneasy_temp["MAP Temperature"], dt=1)

for x in range(len(alphas)):
    pred_sea_levels_nothin[x,:] = gmsl_model(a = alphas[x], Teq=Teqs[x], S0=S0s[x], T_forcing=sneasy_temp["MAP Temperature"], dt=1)

S0 = sneasy_temp["MAP Temperature"][0]

sneasy_temp.iloc[250]

"""Histograms and calculating percentiles to compare the thinned samples for sea level in the year 2100 to the full samples for sea levels."""

idx = 250
plt.hist(pred_sea_levels_thin[:,idx], density=True, alpha = 0.5, bins=20, label="with thinning")
plt.hist(pred_sea_levels_nothin[:,idx], density=True, alpha = 0.5, bins=20, label="no thinning")
plt.legend();

perc_thin = np.percentile(pred_sea_levels_thin[:,idx], [5,50,95])
perc_nothin = np.percentile(pred_sea_levels_nothin[:,idx], [5,50,95])

perc_thin

perc_nothin

